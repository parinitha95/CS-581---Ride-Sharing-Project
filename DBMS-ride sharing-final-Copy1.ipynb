{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googlemaps\n",
    "import math\n",
    "from datetime import datetime,timedelta,date\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to local MySQL db. Replace 'user' with username, 'password' with the specific password and 'dbname' with the name of database created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "engine = create_engine(\"mysql+mysqlconnector://user:\"+'password'+\"@localhost/dbname\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define delay time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delay_time = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of all helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ceil_dt(dt, delta):\n",
    "    dt1 = dt.to_pydatetime()\n",
    "    return dt1 + (datetime.min - dt1) % delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(connection,date1,date2,time1,time2,pool_time):\n",
    "    data = pd.read_sql(\"select * from all_trips where tpep_pickup_datetime >= '\" + date1 + \" \" + time1+\"' and tpep_pickup_datetime <= '\" + date2 + \" \"+ time2+ \"'\", con=connection)\n",
    "#     data['tpep_pickup_datetime'] = pd.to_datetime(data['tpep_pickup_datetime'])\n",
    "#     data['tpep_dropoff_datetime'] = pd.to_datetime(data['tpep_dropoff_datetime'])\n",
    "    data['pickup_timegroup']=pd.to_datetime(data['tpep_pickup_datetime']).apply(lambda x: ceil_dt(x, timedelta(minutes=pool_time)))\n",
    "    data['Trip_ID'] = list(range(1,len(data)+1))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new Google Maps API key which offer 300 dollars for free with a single email ID. Replace 'api' with your api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "with open('data.json') as json_file:  \n",
    "    routes = json.load(json_file)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "API_key = 'api'\n",
    "gmaps = googlemaps.Client(key=API_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get intermediate points between source and destination route from offline data\n",
    "#Returns the intermediate points in reverse direction\n",
    "\n",
    "def getIntermediateLatLong(src, dest):\n",
    "    locations = list()\n",
    "    route = routes[str(src)+'|'+str(dest)]\n",
    "    steps = route['steps']\n",
    "    for step in steps:\n",
    "        loc = (round_half_up(step['maneuver']['location'][0],2),round_half_up(step['maneuver']['location'][1],2))\n",
    "        locations.append(loc)\n",
    "    locations.reverse()\n",
    "    #print(locations)\n",
    "    return locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def newOnWayMerge(merged, unmerged):\n",
    "    merged_new = pd.DataFrame()\n",
    "    unmerged_new = pd.DataFrame()\n",
    "    \n",
    "    def onWayPoolMerge(pool):\n",
    "        nonlocal merged_new,unmerged_new\n",
    "        pool = pool.sort_values(by=['trip_distance'],ascending=False)\n",
    "        #print(pool['trip_distance'])\n",
    "        copy = pool.copy()\n",
    "        delrides = list()\n",
    "        for index1, ride1 in copy.iterrows():\n",
    "\n",
    "            if(ride1['Ride'] not in delrides):\n",
    "                #print('ride 1 - '+str(ride1['Ride']))\n",
    "                delrides.append(ride1['Ride'])\n",
    "                dropoff_loc  = (round_half_up(ride1['dropoff_longitude'],2),round_half_up(ride1['dropoff_latitude'],2))\n",
    "                pickup_loc  = (round_half_up(ride1['pickup_longitude'],2),round_half_up(ride1['pickup_latitude'],2))\n",
    "                \n",
    "                if str(pickup_loc)+'|'+str(dropoff_loc) not in routes:\n",
    "                    continue\n",
    "                \n",
    "                locations = getIntermediateLatLong(pickup_loc,dropoff_loc)\n",
    "\n",
    "                rest = pool[pool['Ride'] != ride1['Ride']]\n",
    "\n",
    "                for index2, ride2 in rest.iterrows():\n",
    "                    if(ride2['Ride'] not in delrides):\n",
    "                        #print('ride 2 - '+str(ride2['Ride']))\n",
    "                        dropoff_loc2  = (round_half_up(ride2['dropoff_longitude'],2),round_half_up(ride2['dropoff_latitude'],2))               \n",
    "                        if(dropoff_loc2 in locations):\n",
    "                            delrides.append(ride2['Ride'])\n",
    "                            #print('merge - '+str(ride1['Ride'])+','+str(ride2['Ride']))\n",
    "                            #print(len(merged_new))\n",
    "                            merged_new = merged_new.append({'pickup_timegroup':ride1['pickup_timegroup'],'Ride_1':ride1['Ride'],'Ride_2':ride2['Ride'],'Ride_1_pickup_datetime':ride1['pickup_datetime'],'Ride_2_pickup_datetime':ride2['pickup_datetime'],'Ride_1_dropoff_datetime':ride1['dropoff_datetime'],'Ride_2_dropoff_datetime':ride2['dropoff_datetime'],'Ride_1_distance':ride1['trip_distance'],'Ride_2_distance':ride2['trip_distance'],'Ride_1_pickup_latitude':ride1['pickup_latitude'],'Ride_2_pickup_latitude':ride2['pickup_latitude'],'Ride_1_pickup_longitude':ride1['pickup_longitude'],'Ride_2_pickup_longitude':ride2['pickup_longitude'],'Ride_1_dropoff_latitude':ride1['dropoff_latitude'],'Ride_2_dropoff_latitude':ride2['dropoff_latitude'],'Ride_1_dropoff_longitude':ride1['dropoff_longitude'],'Ride_2_dropoff_longitude':ride2['dropoff_longitude'],'Ride_1_Time_minutes':ride1['trip_time_minutes'],'Ride_2_Time_minutes':ride2['trip_time_minutes']},ignore_index=True)\n",
    "                            pool.drop([index1,index2],inplace=True )\n",
    "                            break\n",
    "      \n",
    "        for index, rides in pool.iterrows():       \n",
    "            unmerged_new = unmerged_new.append(rides)\n",
    "      \n",
    "    unmerged.groupby(['pool_no']).apply(onWayPoolMerge)\n",
    "    \n",
    "    frames = [merged,merged_new]\n",
    "    totalmerges = pd.concat(frames)\n",
    "    return totalmerges, unmerged_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def round_half_up(n, decimals=0):\n",
    "    multiplier = 10 ** decimals\n",
    "    return math.floor(n*multiplier + 0.5) / multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_algo(processed_data):\n",
    "#     grid = {}\n",
    "    merged = pd.DataFrame()\n",
    "    unmerged = pd.DataFrame()\n",
    "    unmerged1 = pd.DataFrame()\n",
    "    i = 0\n",
    "    def grid_match(x):\n",
    "        nonlocal i,merged,unmerged,unmerged1\n",
    "    #     print(x['pickup_timegroup'])\n",
    "        if i != 0:\n",
    "            d = {}\n",
    "            for _,el in x.iterrows():\n",
    "                key  = (round_half_up(el['dropoff_longitude'],2),round_half_up(el['dropoff_latitude'],2))\n",
    "                if key in d:\n",
    "                    if len(d[key]) > 0:\n",
    "                        ride2 = d[key].pop()\n",
    "                        merged = merged.append({'pickup_timegroup':el['pickup_timegroup'],'Ride_1':el['Trip_ID'],'Ride_2':ride2,'Ride_1_pickup_datetime':el['tpep_pickup_datetime'],'Ride_2_pickup_datetime':processed_data.iloc[ride2-1]['tpep_pickup_datetime'],'Ride_1_dropoff_datetime':el['tpep_dropoff_datetime'],'Ride_2_dropoff_datetime':processed_data.iloc[ride2-1]['tpep_dropoff_datetime'],'Ride_1_distance':el['trip_distance'],'Ride_2_distance':processed_data.iloc[ride2-1]['trip_distance'],'Ride_1_pickup_latitude':el['pickup_latitude'],'Ride_1_pickup_longitude':el['pickup_longitude'],'Ride_2_pickup_latitude':processed_data.iloc[ride2-1]['pickup_latitude'],'Ride_2_pickup_longitude':processed_data.iloc[ride2-1]['pickup_longitude'],'Ride_1_dropoff_latitude':el['dropoff_latitude'],'Ride_2_dropoff_latitude':processed_data.iloc[ride2-1]['dropoff_latitude'],'Ride_1_dropoff_longitude':el['dropoff_longitude'],'Ride_2_dropoff_longitude':processed_data.iloc[ride2-1]['dropoff_longitude'], 'Ride_1_Time_minutes':el['Trip_time_minutes'],'Ride_2_Time_minutes':processed_data.iloc[ride2-1]['Trip_time_minutes']},ignore_index=True)\n",
    "                    else:\n",
    "                        d[key].append(el['Trip_ID'])\n",
    "                else:\n",
    "                    d[key] = [el['Trip_ID']]\n",
    "#             grid[i] = d\n",
    "#             print(d)\n",
    "            for key in d:\n",
    "                if len(d[key])>0:\n",
    "                    ride = d[key].pop()\n",
    "                    unmerged = unmerged.append({'pool_no':i,'pickup_timegroup':processed_data.iloc[ride-1]['pickup_timegroup'],'Ride':ride,'pickup_datetime':processed_data.iloc[ride-1]['tpep_pickup_datetime'],'dropoff_datetime':processed_data.iloc[ride-1]['tpep_dropoff_datetime'],'pickup_latitude':processed_data.iloc[ride-1]['pickup_latitude'],'pickup_longitude':processed_data.iloc[ride-1]['pickup_longitude'],'dropoff_latitude':processed_data.iloc[ride-1]['dropoff_latitude'],'dropoff_longitude':processed_data.iloc[ride-1]['dropoff_longitude'],'trip_time_minutes':processed_data.iloc[ride-1]['Trip_time_minutes'], 'trip_distance':processed_data.iloc[ride-1]['trip_distance']},ignore_index=True)\n",
    "                    unmerged1 = unmerged1.append(processed_data.iloc[ride-1:ride,:])\n",
    "        i += 1\n",
    "    processed_data.groupby(['pickup_timegroup']).apply(grid_match)\n",
    "    return merged,unmerged,unmerged1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def finalgrid(m):\n",
    "#     print(m)\n",
    "    return m.apply(lambda x: process_gridmerge(x['Ride_1'], x['Ride_1_pickup_latitude'], x['Ride_1_pickup_longitude'], x['Ride_1_pickup_datetime'], x['Ride_1_dropoff_latitude'], x['Ride_1_dropoff_longitude'], x['Ride_1_dropoff_datetime'], x['Ride_1_distance'], x['Ride_2'], x['Ride_2_pickup_latitude'], x['Ride_2_pickup_longitude'], x['Ride_2_pickup_datetime'], x['Ride_2_dropoff_latitude'], x['Ride_2_dropoff_longitude'], x['Ride_2_dropoff_datetime'], x['Ride_2_distance']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dist_durat(origins, destination):\n",
    "    a = gmaps.distance_matrix(origins, destination)\n",
    "    #print(a)\n",
    "    distance = a['rows'][0]['elements'][0]['distance']['value']\n",
    "    duration = a['rows'][0]['elements'][0]['duration']['value']\n",
    "    return distance, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def total_run(filtered_values):\n",
    "    if(len(filtered_values) > 1):\n",
    "        filtered_values['key'] = 1\n",
    "        cross_product = pd.merge(filtered_values, filtered_values, how='inner', on = 'key')\n",
    "        allridesmerged = cross_product.loc[(cross_product['ride_nbr_y'] > cross_product['ride_nbr_x'])]\n",
    "#         allridesmerged = euclidean_elimination(allridesmerged)\n",
    "#         print(\"actual end of euclidean elimination\", datetime.now())\n",
    "#         allridesmerged['expected_merged_time'] = allridesmerged.apply(lambda x: (x[\"trip_time_minutes_y\"] - x[\"trip_time_minutes_x\"] + x[\"euclidean_time\"])/x[\"trip_time_minutes_y\"] if(x[\"trip_time_minutes_x\"] < x[\"trip_time_minutes_y\"]) else (x[\"trip_time_minutes_x\"] - x[\"trip_time_minutes_y\"] + x[\"euclidean_time\"])/x[\"trip_time_minutes_x\"], axis = 1)\n",
    "#         a = allridesmerged[['ride_nbr_x','expected_merged_time']].drop_duplicates()\n",
    "#         a.columns = ['new_ride_nbr','expected_merged_time']\n",
    "#         b = allridesmerged[['ride_nbr_y','expected_merged_time']].drop_duplicates()\n",
    "#         b.columns = ['new_ride_nbr','expected_merged_time']\n",
    "#         a=a.append(b)\n",
    "#         a['rank'] = a.groupby('new_ride_nbr')['expected_merged_time'].rank(ascending=False)\n",
    "#         final_ranking = a\n",
    "#         allridesmerged1 = pd.merge(allridesmerged, final_ranking, how=\"left\", left_on = ['ride_nbr_x','expected_merged_time'], right_on = ['new_ride_nbr','expected_merged_time'])\n",
    "#         allridesmerged2 = pd.merge(allridesmerged1, final_ranking, how=\"left\", left_on = ['ride_nbr_y','expected_merged_time'], right_on = ['new_ride_nbr','expected_merged_time'])\n",
    "#         allridesmerged2 = allridesmerged2[(allridesmerged2['rank_x'] <= 3) & (allridesmerged2['rank_y'] <= 3)]\n",
    "#         allridesmerged = allridesmerged2.iloc[:, :allridesmerged2.shape[1]-4]\n",
    "        listofmergedrides=allridesmerged.apply(lambda x: fair_share_merge(x['ride_nbr_x'], x['pickup_latitude_x'], x['pickup_longitude_x'], x['tpep_pickup_datetime_x'], x['dropoff_latitude_x'], x['dropoff_longitude_x'], x['tpep_dropoff_datetime_x'], x['trip_distance_x'], x['ride_nbr_y'], x['pickup_latitude_y'], x['pickup_longitude_y'], x['tpep_pickup_datetime_y'], x['dropoff_latitude_y'], x['dropoff_longitude_y'], x['tpep_dropoff_datetime_y'], x['trip_distance_y']), axis = 1)\n",
    "        return merging_rides(listofmergedrides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0;\n",
    "def process_gridmerge(ride_nbra, picklata, picklonga, pickupta, droplata, droplonga, dropta, dista, ride_nbrb, picklatb, picklongb, pickuptb, droplatb, droplongb, droptb, distb):\n",
    "    global count\n",
    "    count+=1\n",
    "    #print(count)\n",
    "#     print(\"calling processed grid merge\")\n",
    "    org_durata = int((dropta - pickupta).total_seconds())    \n",
    "    org_duratb = int((droptb - pickuptb).total_seconds())\n",
    "    \n",
    "    #dist1, durat1 = get_dist_durat((picklata, picklonga),(picklatb, picklongb))\n",
    "    #dist2, durat2 = get_dist_durat((picklatb, picklongb),(droplata, droplonga))\n",
    "    if(dista <= distb):\n",
    "        #dist3, durat3 = get_dist_durat((droplata, droplonga),(droplatb, droplongb))\n",
    "        totdist_option1 = distb * (1 + 0.1)\n",
    "        totdurat_a_option1 = org_durata\n",
    "        totdurat_b_option1 = org_durata * (1 + 0.1)\n",
    "    else:\n",
    "        #dist3, durat3 = get_dist_durat((droplatb, droplongb),(droplata, droplonga))\n",
    "        totdist_option1 = dista * (1 + 0.1)\n",
    "        totdurat_a_option1 = org_duratb * (1 + 0.1)\n",
    "        totdurat_b_option1 = org_duratb\n",
    "    #if((org_durata * (1 + delay_time) >= totdurat_a_option1) & (org_duratb * (1 + delay_time) >= totdurat_b_option1)):\n",
    "    return (ride_nbra, ride_nbrb, ride_nbra, ride_nbrb, dista, distb, totdist_option1, (dista + distb) - totdist_option1, org_durata, org_duratb, totdurat_a_option1, totdurat_b_option1)\n",
    "    #else:\n",
    "    #    return None, None, None, None, None, None, None, None, None, None, None, None;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Euclidean Distance\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "\n",
    "def euclidean_dist(lata, longa, latb, longb):\n",
    "    # approximate radius of earth in km\n",
    "    R = 6373.0\n",
    "    lat1 = radians(lata)\n",
    "    lon1 = radians(longa)\n",
    "    lat2 = radians(latb)\n",
    "    lon2 = radians(longb)\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "\n",
    "    return distance/1.609"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Euclidean Elimination\n",
    "ğ‘†ğ‘ƒ(ğ´) + ğ‘‡(ğ‘‘ğ‘’ğ‘ ğ‘¡ ğ´ , ğ‘‘ğ‘’ğ‘ ğ‘¡(ğµ)) < ğ‘†ğ‘ƒ(ğµ) + ğ·elay(ğµ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclidean_elimination(allridesmerged):\n",
    "    allridesmerged['euclidean_a_b'] = allridesmerged.apply(lambda x: euclidean_dist(x[\"dropoff_latitude_x\"], x[\"dropoff_longitude_x\"], x[\"dropoff_latitude_y\"], x[\"dropoff_longitude_y\"]), axis = 1)\n",
    "    print(\"euclidean distance time\", datetime.now())\n",
    "    #allridesmerged['trip_time_x'] = allridesmerged.apply(lambda x: int((datetime.strptime(x[\"tpep_dropoff_datetime_x\"], '%Y-%m-%d %H:%M:%S') - datetime.strptime(x[\"tpep_pickup_datetime_x\"], '%Y-%m-%d %H:%M:%S')).total_seconds()), axis=1)\n",
    "    #allridesmerged['trip_time_y'] = allridesmerged.apply(lambda x: int((datetime.strptime(x[\"tpep_dropoff_datetime_y\"], '%Y-%m-%d %H:%M:%S') - datetime.strptime(x[\"tpep_pickup_datetime_y\"], '%Y-%m-%d %H:%M:%S')).total_seconds()), axis=1)\n",
    "    #allridesmerged['trip_time_x'] = allridesmerged.apply(lambda x: int((x[\"tpep_dropoff_datetime_x\"] - x[\"tpep_pickup_datetime_x\"]).total_seconds()), axis=1)\n",
    "    #print(\"time1\", datetime.now())\n",
    "    #allridesmerged['trip_time_y'] = allridesmerged.apply(lambda x: int((x[\"tpep_dropoff_datetime_y\"] - x[\"tpep_pickup_datetime_y\"]).total_seconds()), axis=1)\n",
    "    #print(\"time2\", datetime.now())\n",
    "    allridesmerged['average_speed'] = (allridesmerged['trip_distance_x']/allridesmerged['trip_time_minutes_x'] + allridesmerged['trip_distance_y']/allridesmerged['trip_time_minutes_y'])/2\n",
    "    allridesmerged['euclidean_time'] = allridesmerged['euclidean_a_b']/(allridesmerged['average_speed'])\n",
    "    #allridesmerged['flag'] = allridesmerged.apply(lambda x: 1 if((x[\"trip_time_minutes_x\"] + x[\"euclidean_time\"] <= x[\"trip_time_minutes_y\"] * (1 + delay_time)) |\n",
    "    #                                                        (x[\"trip_time_minutes_y\"] + x[\"euclidean_time\"] <= x[\"trip_time_minutes_x\"] * (1 + delay_time))) else 0, axis = 1)\n",
    "    \n",
    "    return allridesmerged.loc[(allridesmerged[\"trip_time_minutes_x\"] + allridesmerged[\"euclidean_time\"] <= allridesmerged[\"trip_time_minutes_y\"] * (1 + delay_time))|(allridesmerged[\"trip_time_minutes_y\"] + allridesmerged[\"euclidean_time\"] <= allridesmerged[\"trip_time_minutes_x\"] * (1 + delay_time))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "def fair_share_merge(ride_nbra, picklata, picklonga, pickupta, droplata, droplonga, dropta, dista, ride_nbrb, picklatb, picklongb, pickuptb, droplatb, droplongb, droptb, distb):\n",
    "    global count\n",
    "    count+=1\n",
    "    org_durata = int((dropta - pickupta).total_seconds())\n",
    "    org_duratb = int((droptb - pickuptb).total_seconds())\n",
    "    if(org_durata <= org_duratb):\n",
    "        dist3, durat3 = get_dist_durat((droplata, droplonga),(droplatb, droplongb)) \n",
    "        totdist_option1 = dista + ((dist3/1000)/1.609)\n",
    "        totdurat_a_option1 = org_durata\n",
    "        totdurat_b_option1 = org_durata + durat3\n",
    "        option1 = [totdist_option1, totdurat_a_option1, totdurat_b_option1]\n",
    "    else:\n",
    "        dist3, durat3 = get_dist_durat((droplatb, droplongb),(droplata, droplonga))\n",
    "        totdist_option1 = distb + ((dist3/1000)/1.609)\n",
    "        totdurat_a_option1 = org_duratb + durat3\n",
    "        totdurat_b_option1 = org_duratb\n",
    "        option1 = [totdist_option1, totdurat_a_option1, totdurat_b_option1]\n",
    "    option1_dist = checktimecriteria(org_durata, org_duratb, option1)\n",
    "    if(option1_dist != 1000000):\n",
    "        return ride_nbra, ride_nbrb, ride_nbra, ride_nbrb, dista, distb, totdist_option1, org_durata, org_duratb, totdurat_a_option1, totdurat_b_option1;\n",
    "    else:\n",
    "        return None, None, None, None, None, None, None, None, None, None, None;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getbestpossiblemerge(org_durata, org_duratb, option1, option2):\n",
    "    option1_dist = checktimecriteria(org_durata, org_duratb, option1)\n",
    "    option2_dist = checktimecriteria(org_durata, org_duratb, option2)\n",
    "    values = [option1_dist, option2_dist]\n",
    "    #print(values)\n",
    "    #print(min(values))\n",
    "    if min(values) != 1000000:\n",
    "        return values.index(min(values))+1\n",
    "    else:\n",
    "        return 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checktimecriteria(org_durata, org_duratb, option):\n",
    "    #print(\"org_durata\", org_durata*(1+delay_time), option[1])\n",
    "    #print(\"org_duratb\", org_duratb*(1+delay_time), option[2])    \n",
    "    if((org_durata*(1+delay_time) >= option[1]) & (org_duratb*(1+delay_time) >= option[2])):\n",
    "        return option[0]\n",
    "    else:\n",
    "        return 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merging_rides(a):\n",
    "    final_database = pd.DataFrame(list(a))\n",
    "    final_database.columns = ['first_pickup_ride','second_pickup_ride','first_drop_ride','second_drop_ride','first_dist_org','second_dist_org','merged_dist','first_durat_org','second_durat_org', 'first_durat_new','second_durat_new']\n",
    "    final_database = final_database.loc[~final_database['first_pickup_ride'].isna()]\n",
    "    final_database['merged_dist'] = final_database['merged_dist']\n",
    "    final_database['dist_saved'] = (final_database['first_dist_org'] + final_database['second_dist_org']) - final_database['merged_dist']\n",
    "    highest = pd.DataFrame(final_database.groupby(['first_pickup_ride'])['dist_saved'].max().reset_index())\n",
    "    highest.columns = ['ride', 'dist_saved']\n",
    "    sec_highest = pd.DataFrame(final_database.groupby(['second_pickup_ride'])['dist_saved'].max().reset_index())\n",
    "    sec_highest.columns = ['ride','dist_saved']\n",
    "    new_highest = highest.append(sec_highest)\n",
    "    new_highest = new_highest.groupby('ride')['dist_saved'].max().reset_index()\n",
    "    final_database_new = pd.merge(final_database, new_highest, how = \"inner\", left_on = ['first_pickup_ride'], right_on = ['ride'])\n",
    "    final_database_new.rename(index=str, columns={\"dist_saved_y\": \"first_max_dist\"}, inplace = True)\n",
    "    final_database_inter = pd.merge(final_database_new, new_highest, how = \"inner\", left_on = ['second_pickup_ride'], right_on = ['ride'])\n",
    "    final_database_inter.rename(index=str, columns={\"dist_saved_x\": \"merged_dist_saved\", \"dist_saved\": \"second_max_dist\"}, inplace = True)\n",
    "    #return final_database_inter\n",
    "    final_merged_rides = final_database_inter.loc[(final_database_inter['merged_dist_saved'] == final_database_inter['first_max_dist']) &\n",
    "                        (final_database_inter['merged_dist_saved'] == final_database_inter['second_max_dist'])]\n",
    "    return final_merged_rides[['first_pickup_ride','second_pickup_ride','first_drop_ride','second_drop_ride','first_dist_org','second_dist_org','merged_dist','merged_dist_saved','first_durat_org','second_durat_org', 'first_durat_new','second_durat_new']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below cell runs the entire algorithm. Pooling window, start and end dates, start and end times can be modified as needed. Start and end dates as well as start and end times refers to the dataset to be considered for the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pooling_window = 5\n",
    "start_date = \"2016-01-01\"\n",
    "end_date = \"2016-02-29\"\n",
    "start_time = \"00:00:00\"\n",
    "end_time = \"24:00:00\"\n",
    "\n",
    "print(\"Start\", datetime.now())\n",
    "processed_data = preprocess(engine,start_date,end_date,start_time,end_time,pooling_window)\n",
    "print(\"Preprocess done\", datetime.now())\n",
    "iteration = 0\n",
    "merged, unmerged, grid_processed_rem = grid_algo(processed_data)\n",
    "merged_new, unmerged_new = newOnWayMerge(merged, unmerged)\n",
    "merged_new = merged_new.reset_index()\n",
    "unmerged_new.rename(index=str, columns={\"Ride\": \"ride_nbr\", \"pickup_datetime\": \"tpep_pickup_datetime\",\n",
    "                                       \"dropoff_datetime\": \"tpep_dropoff_datetime\"}, inplace = True)\n",
    "unmerged_new = unmerged_new.drop_duplicates()\n",
    "print(\"Ride - Grid Mapping done\", datetime.now())\n",
    "print(\"Total No: of Timegroup pools\", len(processed_data[\"pickup_timegroup\"].unique()))\n",
    "poolcount = 0\n",
    "# print(processed_data[\"pickup_timegroup\"].unique())\n",
    "for a in processed_data[\"pickup_timegroup\"].unique():\n",
    "    if(iteration == 0):\n",
    "        if(len(merged_new[merged_new[\"pickup_timegroup\"] == a]) > 0):\n",
    "            print(merged_new[merged_new[\"pickup_timegroup\"] == a].shape)\n",
    "            a1 = finalgrid(merged_new[merged_new[\"pickup_timegroup\"] == a])\n",
    "            final_database = pd.DataFrame(list(a1))  \n",
    "            final_database.columns = ['first_pickup_ride','second_pickup_ride','first_drop_ride','second_drop_ride','first_dist_org','second_dist_org','merged_dist','merged_dist_saved','first_durat_org','second_durat_org', 'first_durat_new','second_durat_new']\n",
    "            final_database = final_database.loc[~final_database['first_pickup_ride'].isna()]\n",
    "        final_data = total_run(unmerged_new.loc[unmerged_new[\"pickup_timegroup\"] == a])\n",
    "    else:\n",
    "        if(len(merged_new[merged_new[\"pickup_timegroup\"] == a]) > 0):\n",
    "            a1 = finalgrid(merged_new[merged_new[\"pickup_timegroup\"] == a])\n",
    "            final_database1 = pd.DataFrame(list(a1))\n",
    "            final_database1.columns = ['first_pickup_ride','second_pickup_ride','first_drop_ride','second_drop_ride','first_dist_org','second_dist_org','merged_dist','merged_dist_saved','first_durat_org','second_durat_org', 'first_durat_new','second_durat_new']\n",
    "            final_database = final_database.append(final_database1.loc[~final_database1['first_pickup_ride'].isna()])\n",
    "        final_data = final_data.append(total_run(unmerged_new.loc[unmerged_new[\"pickup_timegroup\"] == a]))\n",
    "    iteration += 1\n",
    "    print(\"Pool count: \", iteration)\n",
    "    print(datetime.now())\n",
    "print(\"Merges post Grid, Euclidean elimination & Fair Matching done\", datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the final data as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_data = final_database.append(final_data)\n",
    "final_data.to_csv('final_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Plot for Average % of trips saved per pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as mpl\n",
    "xpoints = ['5 mins']\n",
    "y_pos = [0]\n",
    "# print(y_pos)\n",
    "avg_trips_sav = ((len(final_data)) / len(processed_data)) * 100\n",
    "print(avg_trips_sav)\n",
    "y_points = [avg_trips_sav]\n",
    "mpl.bar(y_pos, y_points, align='center', alpha=0.4,width=0.1)\n",
    "mpl.xticks(y_pos, xpoints)\n",
    "mpl.ylabel('Average % trips saved')\n",
    "mpl.title('Average number of trips saved per pool as a % of the number of individual trips')\n",
    "mpl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot for average % of distance saved per pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as mpl\n",
    "xpoints = ['5 mins']\n",
    "y_pos = [0]\n",
    "total_ind_dist = sum(processed_data['trip_distance'])\n",
    "av_mil_save = sum(final_data['merged_dist_saved'])\n",
    "av_dist_saved = (av_mil_save/ total_ind_dist)*100\n",
    "y_points = [av_dist_saved]\n",
    "mpl.bar(y_pos, y_points, align='center', alpha=0.4,width=0.1)\n",
    "mpl.xticks(y_pos, xpoints)\n",
    "mpl.ylabel('Average % distance saved')\n",
    "mpl.title('Average distance saved per pool as a % of the total distance of individual (unshared) trips')\n",
    "mpl.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
